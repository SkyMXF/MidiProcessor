{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Zip Midi List and Clean\n",
    "Generate a list of midi files in a zip file, and clean (remove invalid files and duplicated files)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "zip_file_path = '../../data/merged-dataset.zip'\n",
    "suffixes = ('mid', )\n",
    "midi_checker = 'default'\n",
    "remove_invalid = True\n",
    "remove_duplicated = True\n",
    "save_path = '../../processed_data/info_note/merged-dataset/zip.merged-dataset.clean.deduplicated.txt'\n",
    "num_workers=None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "from midiprocessor import midi_utils, data_utils\n",
    "import zipfile\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "file_path_list = data_utils.get_zip_file_paths(zip_file_path, suffixes=suffixes)\n",
    "num_files = len(file_path_list)\n",
    "print('%d files' % num_files)\n",
    "for item in file_path_list[:10]:\n",
    "    print(item)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2111510 files\n",
      "bitmidi.com/000001.mid\n",
      "bitmidi.com/000002.mid\n",
      "bitmidi.com/000003.mid\n",
      "bitmidi.com/000004.mid\n",
      "bitmidi.com/000005.mid\n",
      "bitmidi.com/000006.mid\n",
      "bitmidi.com/000007.mid\n",
      "bitmidi.com/000008.mid\n",
      "bitmidi.com/000009.mid\n",
      "bitmidi.com/000010.mid\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def batch_process_zip(zip_file_path, file_path_list, midi_checker='default', order=0):\n",
    "    file_list = set()\n",
    "    invalid_file_list = set()\n",
    "    md5_dict = defaultdict(list)\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_obj:\n",
    "        for file_path in tqdm(file_path_list, position=order):\n",
    "            try:\n",
    "                with zip_obj.open(file_path, 'r') as f:\n",
    "                    md5 = data_utils.get_md5_sum(file_obj=f)\n",
    "                    md5_dict[md5].append(file_path)\n",
    "\n",
    "                    f.seek(0)\n",
    "                    midi_obj = midi_utils.load_midi(file=f, midi_checker=midi_checker)\n",
    "            except:\n",
    "                invalid_file_list.add(file_path)\n",
    "                # Todo\n",
    "                # if not remove_invalid:\n",
    "                #     file_list.append(relative_path)\n",
    "            else:\n",
    "                file_list.add(file_path)\n",
    "\n",
    "    return file_list, invalid_file_list, md5_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def split_list(file_path_list, num):\n",
    "    each_length = num_files // num_workers + 1\n",
    "    batch_file_lists = []\n",
    "\n",
    "    left = 0\n",
    "    while True:\n",
    "        right = min(num_files, left + each_length)\n",
    "        temp_file_list = file_path_list[left: right]\n",
    "        batch_file_lists.append(temp_file_list)\n",
    "        left = right\n",
    "        if left >= num_files:\n",
    "            break\n",
    "    assert len(batch_file_lists) == num\n",
    "    \n",
    "    return batch_file_lists"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def process(zip_file_path, file_path_list, midi_checker='default', remove_invalid=True, remove_duplicated=True, num_workers=None):\n",
    "    final_valid = []\n",
    "    final_invalid = []\n",
    "    duplicated_groups = []\n",
    "    final_md5_dict = None\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as pool:\n",
    "        if num_workers is None:\n",
    "            num_workers = pool._max_workers\n",
    "        print('Using %d to process %d files...' % (num_workers, len(file_path_list)))\n",
    "        \n",
    "        batch_file_lists = split_list(file_path_list, num_workers)\n",
    "\n",
    "        results = pool.map(\n",
    "            batch_process_zip,\n",
    "            [zip_file_path] * num_workers,\n",
    "            batch_file_lists,\n",
    "            [midi_checker] * num_workers,\n",
    "            range(num_workers),\n",
    "        )\n",
    "\n",
    "    for file_list, invalid_file_list, md5_dict in results:\n",
    "        final_valid.extend(list(file_list))\n",
    "        final_invalid.extend(list(invalid_file_list))\n",
    "        if final_md5_dict is None:\n",
    "            final_md5_dict = md5_dict\n",
    "        else:\n",
    "            for key in md5_dict:\n",
    "                final_md5_dict[key].extend(md5_dict[key])\n",
    "\n",
    "    if remove_invalid:\n",
    "        final_list = final_valid\n",
    "    else:\n",
    "        final_list = final_valid + final_invalid\n",
    "    final_list = set(final_list)\n",
    "    \n",
    "    if remove_duplicated:\n",
    "        for key in final_md5_dict:\n",
    "            if len(final_md5_dict) > 1:\n",
    "                group = final_md5_dict[key]\n",
    "                duplicated_groups.append(group)\n",
    "                for item in group[1:]:\n",
    "                    try:\n",
    "                        final_list.remove(item)\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "    \n",
    "    return final_list, final_invalid, duplicated_groups"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "final_list, final_invalid, duplicated_groups = process(zip_file_path, file_path_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using 8 to process 2111510 files...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_utils.dump_list(file_list, save_path)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ff0ca1643464aa02e32831549484bc5426b273aafa0ac5ed8478245e429ee9b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}